{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 공통 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import uuid\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import http\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. API 키 발급 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CLOVASTUDIO_API_KEY\"] = getpass.getpass(\"CLOVA Studio API Key: \") # 나중에 청킹화 할 때 필요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PDF 문서에서 텍스트와 이미지 추출하기 (Load)\n",
    "\n",
    "이미지 추출이 있을 수는 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import glob\n",
    "from langchain_core.documents import Document\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_documents_from_pdf(pdf_path: str, output_dir: str = \"data/extracted_images_문서\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    merged_text_path = os.path.join(output_dir, \"merged_text\" + Path(pdf_path).stem + \".txt\")\n",
    "    merged_text = \"\"\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    documents = []\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        page_number = i + 1\n",
    "        page_text = page.get_text(\"text\").strip()\n",
    "        images_info = []\n",
    "\n",
    "        # 이미지 추출\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image_filename = f\"page_{page_number}_img_{img_index+1}.{image_ext}\"\n",
    "            image_path = os.path.join(output_dir, image_filename)\n",
    "\n",
    "            with open(image_path, \"wb\") as img_file:\n",
    "                img_file.write(image_bytes)\n",
    "\n",
    "            images_info.append(image_path)\n",
    "\n",
    "        # LangChain Document로 변환\n",
    "        documents.append(Document(\n",
    "            page_content=page_text,\n",
    "            metadata={\n",
    "                \"source\": os.path.basename(pdf_path),\n",
    "                \"page\": page_number,\n",
    "                \"images\": \", \".join(images_info)\n",
    "            }\n",
    "        ))\n",
    "\n",
    "        # 병합 텍스트 저장용\n",
    "        merged_text += f\"\\n\\n--- Page {page_number} ---\\n\\n{page_text}\"\n",
    "\n",
    "    # 전체 텍스트 저장\n",
    "    with open(merged_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(merged_text)\n",
    "\n",
    "    return documents, merged_text_path\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "folder_path = os.path.join(current_dir, \"data\")\n",
    "# print( os.path.join(os.getcwd(), \"data\"))\n",
    "pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n",
    "\n",
    "docs = []\n",
    "print(f\"PDF 파일 개수: {len(pdf_files)}\")\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = pdf_file\n",
    "    temp_docs, merged_path = extract_documents_from_pdf(pdf_file)\n",
    "\n",
    "    print(f\"추출된 문서 페이지 수: {len(temp_docs)}\")\n",
    "    print(f\"병합된 텍스트 경로: {merged_path}\")\n",
    "    print(temp_docs[0])  # 하나 확인\n",
    "\n",
    "    docs.extend(temp_docs)  # 전체 문서 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 클라우드에서 발급받은 키를 입력하세요\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = getpass.getpass(\"NCP Access Key: \")\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = getpass.getpass(\"NCP Secret Key: \")\n",
    "\n",
    "# 기본 리전 설정\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"kr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_naver import ChatClovaX\n",
    "\n",
    "chat_llm = ChatClovaX(\n",
    "    model=\"HCX-005\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "class CompletionExecutor:\n",
    "    def __init__(self, host, api_key, request_id):\n",
    "        self._host = host\n",
    "        self._api_key = api_key\n",
    "        self._request_id = request_id\n",
    "\n",
    "    def _send_request(self, completion_request):\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json; charset=utf-8',\n",
    "            'Authorization': self._api_key,\n",
    "            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id\n",
    "        }\n",
    "\n",
    "        conn = http.client.HTTPSConnection(self._host)\n",
    "        conn.request('POST', '/testapp/v1/api-tools/segmentation', json.dumps(completion_request), headers)\n",
    "        response = conn.getresponse()\n",
    "        result = json.loads(response.read().decode(encoding='utf-8'))\n",
    "        conn.close()\n",
    "        return result\n",
    "\n",
    "    def execute(self, completion_request):\n",
    "        res = self._send_request(completion_request)\n",
    "        if res['status']['code'] == '20000':\n",
    "            return res['result']['topicSeg']\n",
    "        else:\n",
    "            print(\"[CLOVA 응답 오류]\", res['status'])\n",
    "            return 'Error'\n",
    "        \n",
    "# file_path = \"data/extracted_images_문서/merged_text.txt\"\n",
    "\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     text_content = f.read()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    completion_executor = CompletionExecutor(\n",
    "        host='clovastudio.stream.ntruss.com',\n",
    "        api_key=\"Bearer \"+os.environ[\"CLOVASTUDIO_API_KEY\"], # 여기 키 형식이 Bearer이 붙네요 \n",
    "        request_id=str(uuid.uuid4())\n",
    "    )\n",
    "\n",
    "    chunked_docs = []\n",
    "\n",
    "    for doc in docs:  # docs는 페이지별로 추출한 Document 리스트\n",
    "        segments = completion_executor.execute(\n",
    "            # 이전 블로그 참고해 파라미터 설정\n",
    "            {\"postProcessMaxSize\": 100,   # 후처리 시 하나의 문단이 가질 수 있는 최대 글자 수 (예: 1000자 이하로 잘라줌)\n",
    "            \"alpha\": -100,                # 문단 나누기 민감도 조절 파라미터 (기본: 0.0 / -100으로 두면 자동 조정) - 값이 클수록 더 잘게 나뉘고, 작을수록 덜 나뉨\n",
    "            \"segCnt\": -1,                 # 원하는 문단 개수 설정 (-1이면 자동 분할, 1 이상의 정수 입력 시 해당 개수로 고정)\n",
    "            \"postProcessMinSize\": -1,     # 후처리 시 하나의 문단이 가져야 할 최소 글자 수 (예: 300자 이상 유지)\n",
    "            \"text\": doc.page_content,     # 실제 분할할 원본 텍스트\n",
    "            \"postProcess\": True}          # 후처리 여부 설정 (True: 문단 길이 균일화 / False: 모델 출력 그대로 사용)\n",
    "        )\n",
    "\n",
    "    for seg in segments:\n",
    "        chunked_docs.append(Document(\n",
    "            page_content=' '.join(seg),\n",
    "            metadata=doc.metadata\n",
    "        ))    \n",
    "\n",
    "    print(chunked_docs)\n",
    "    print(\"chunk 개수 :\",len(chunked_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_docs를 chunked_docs에 추가 (원본은 그대로 유지) \n",
    "# 원래 image 정보도 같이 받았는데 안 받는 것으로.\n",
    "combined_docs = chunked_docs\n",
    "\n",
    "print(f\"전체 chunk 개수: {len(combined_docs)}\")\n",
    "print(combined_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 청크 출력\n",
    "print(\"\\n샘플 청크 (처음 3개):\")\n",
    "for i, chunk in enumerate(combined_docs[:3], 0):\n",
    "    print(f\"\\n청크 {i+1}:\")\n",
    "    print(f\"내용: {chunk.page_content}\")\n",
    "    print(f\"metadata: {chunk.metadata}\")\n",
    "    print(f\"길이: {len(chunk.page_content)}자\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_naver import ClovaXEmbeddings\n",
    " \n",
    "clovax_embeddings = ClovaXEmbeddings(model='bge-m3') # 임베딩 모델을 설정\n",
    "\n",
    "text = \"임베딩 사용 예제입니다~\"\n",
    " \n",
    "clovax_embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "# 임베딩 모델 정의\n",
    "clovax_embeddings = ClovaXEmbeddings(model='bge-m3')\n",
    "\n",
    "# 로컬 클라이언트 생성\n",
    "client = chromadb.PersistentClient(path=\"./Chroma_langchain_db123\")\n",
    "\n",
    "# 컬렉션 준비 (이름 중복 주의!)\n",
    "collection_name = \"clovastudiodatas_docs\"\n",
    "client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# 벡터스토어 객체 생성\n",
    "vectorstore_Chroma = Chroma(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=clovax_embeddings\n",
    ")\n",
    "\n",
    "# 문서 추가: 최신 방식은 vectorstore.add_documents 사용\n",
    "print(\"Adding documents to Chroma vectorstore...\")\n",
    "for doc in combined_docs:\n",
    "    try:\n",
    "        vectorstore_Chroma.add_documents([doc])\n",
    "        time.sleep(1.1) \n",
    "    except Exception as e:\n",
    "        print(f\"[✘] 실패: {doc.metadata} → {e}\")\n",
    "\n",
    "print(\"All documents have been added to the vectorstore.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FAISS 다운로드\n",
    "%pip install -qU langchain-community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# 임베딩 모델 정의\n",
    "clovax_embeddings = ClovaXEmbeddings(model='bge-m3')\n",
    "\n",
    "# FAISS 인덱스 생성 (1024는 bge-m3 차원 수에 맞춰야 함)\n",
    "index = faiss.IndexFlatIP(1024)  # 내적 기반 검색\n",
    "\n",
    "# FAISS 벡터스토어 생성\n",
    "vectorstore_FAISS = FAISS(\n",
    "    embedding_function=clovax_embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# 문서 일괄 추가 (자동 임베딩 처리)\n",
    "print(\"Adding documents to FAISS vectorstore...\")\n",
    "for doc in combined_docs:\n",
    "    try:\n",
    "        vectorstore_FAISS.add_documents([doc])\n",
    "        time.sleep(1.1) \n",
    "    except Exception as e:\n",
    "        print(f\"[✘] 실패: {doc.metadata} → {e}\")\n",
    "print(\"All documents have been added to FAISS vectorstore.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 질의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# System 및 User 메시지를 나눠 구성\n",
    "system_template = (\n",
    "    \"당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 원래 가지고있는 지식은 모두 배제하고, 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.\"\n",
    "    \"만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.\"\n",
    ")\n",
    "user_template = (\n",
    "    \"다음은 검색된 문서 내용입니다:\\n\\n{context}\\n\\n\"\n",
    "    \"위 정보를 바탕으로 다음 질문에 답해주세요:\\n{question}\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template),\n",
    "])\n",
    "\n",
    "# 원하는 vectorstore 선택해서 사용\n",
    "# retriever = vectorstore_Chroma.as_retriever(\n",
    "#     search_type=\"similarity_score_threshold\",\n",
    "#     search_kwargs={\"score_threshold\": 0.1, \"k\": 3}\n",
    "#     )\n",
    "retriever = vectorstore_FAISS.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.1, \"k\": 3}\n",
    ")\n",
    "\n",
    "# Retrieval QA 체인 구성\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 실행\n",
    "question = \"증빙소득의 연소득 산정방법을 알고 싶어\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "print(\"질문:\", question)\n",
    "print(\"응답:\", result[\"result\"])  # 모델의 실제 응답\n",
    "for i, doc in enumerate(result[\"source_documents\"]): # 답변시 참고 한 문서\n",
    "    print(f\"\\n[출처 문서 {i+1}]\\n내용: {doc.page_content}\\n메타데이터: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

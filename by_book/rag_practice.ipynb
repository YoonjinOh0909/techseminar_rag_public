{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807749a0",
   "metadata": {},
   "source": [
    "## PyPDFLoader로 pdf 읽기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5801b44f",
   "metadata": {},
   "source": [
    "### data에 있는 모든 문서 추출하기\n",
    "- chroma_store가 없을 경우 data 폴더에 있는 pdf 파일들을 읽어서 벡터 DB에 저장 하기 위한 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84d850",
   "metadata": {},
   "source": [
    "### 청킹 방법 선택하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c88d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청킹 방법\n",
    "splitter_name = \"sementic\" # recursive, sementic\n",
    "\n",
    "# 임베딩 모델\n",
    "MODEL = \"text-embedding-3-large\" # text-embedding-3-large\n",
    "\n",
    "# LLM 모델\n",
    "LLM_MODEL = \"gpt-4o-mini\" # gpt-4o-mini, openai/gpt-oss-120b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0503fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if splitter_name == \"recursive\" :\n",
    "    # recursive_character_text_splitter로 구분자 재귀 청킹 기법\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "elif splitter_name == \"semantic\":\n",
    "    import os\n",
    "    from langchain_experimental.text_splitter import SemanticChunker\n",
    "    from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    # 의미 기반으로 청킹을 하기 위해 OpenAI의 임베딩 모델을 사용\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    embedding = OpenAIEmbeddings(model=MODEL, api_key=OPENAI_API_KEY)\n",
    "\n",
    "    text_splitter = SemanticChunker(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c63422",
   "metadata": {},
   "source": [
    "### Debug 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f42103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_chunkinfo_aftersplit(all_splits):\n",
    "    for i, split in enumerate(all_splits):\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        print(split.page_content)\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17918d6",
   "metadata": {},
   "source": [
    "### 추출 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf92787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일을 읽어서 텍스트 데이터 추출 및 청킹\n",
    "def extract_documents_from_pdf(pdf_path):\n",
    "    # PDF 파일을 읽어서 텍스트 데이터 추출\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    data_nyc = loader.load()\n",
    "\n",
    "    # 추출된 텍스트 데이터를 청킹\n",
    "    splits = text_splitter.split_documents(data_nyc)\n",
    "    # debug_chunkinfo_aftersplit(splits) \n",
    "\n",
    "    # recursive_character_text_splitter의 경우, 청크가 겹치는 부분이 없으면 연결하지 않음\n",
    "    # 따라서, 청크가 겹치는 부분이 없을 때는 직접 연결하여 오버랩을 만듦\n",
    "    # 만약 청크가 겹치는 부분이 있다면, 그 부분은 자동으로 연결됨\n",
    "    # 여기서는 청크가 겹치지 않는 경우에만 오버랩을 추가함\n",
    "\n",
    "    # 만약 첫 번째 청크의 끝과 두 번째 청크의 시작이 겹치지 않는다면,\n",
    "    if splitter_name == \"recursive\" and (splits[0].page_content[-100:] == splits[1].page_content[:100]):\n",
    "        print(splits[0].page_content)\n",
    "        print(\"----\")\n",
    "        print(splits[1].page_content)\n",
    "        for i in range(len(splits) - 1):\n",
    "            splits[i].page_content += \"\\n\" + splits[i + 1].page_content[:50]\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "current_dir = os.getcwd() # 현재 폴더 경로\n",
    "folder_path = os.path.join(current_dir, \"data\") # data 폴더 경로 설정\n",
    "pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\")) # PDF 파일 목록 가져오기\n",
    "\n",
    "all_splits = []\n",
    "print(f\"현재 청킹 방법 : {splitter_name}\")\n",
    "print(f\"현재 모델 : {MODEL}\")\n",
    "print(f\"PDF 파일 개수: {len(pdf_files)}\")\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = pdf_file\n",
    "    # temp_docs, merged_path = extract_documents_from_pdf(pdf_file)\n",
    "    all_splits.extend(extract_documents_from_pdf(pdf_file))\n",
    "\n",
    "print(f\"전체 청크 개수: {len(all_splits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87940ce",
   "metadata": {},
   "source": [
    "## 텍스트를 벡터로 변환하기\n",
    "- 임베딩 모델 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "if splitter_name != \"semantic\":\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    # text-embedding-3-small이 비용 대비 성능이 매우 뛰어나지만, 한글 문서에서는 text-embedding-3-large가 더 나은 성능을 보여 large로 선택.\n",
    "    MODEL = \"text-embedding-3-large\"\n",
    "    embedding = OpenAIEmbeddings(model=MODEL, api_key=OPENAI_API_KEY)\n",
    "\n",
    "# 예시\n",
    "# v = embedding.embed_query(\"뉴욕의 온실가스 저감정책은 뭐야?\")\n",
    "# print(v)\n",
    "# print(len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f24cc",
   "metadata": {},
   "source": [
    "## 크로마 DB 생성하고 데이터 불러오기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e9be1",
   "metadata": {},
   "source": [
    "- 최초에 chrom_store을 만들 때 data에 있는 pdf를 사용해서 생성할 때."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f077e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import tiktoken\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "persist_directory = './chroma_store' + '_' + splitter_name + '_' + MODEL.replace('.', '_')\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(MODEL)\n",
    "\n",
    "TOKEN_LIMIT_PER_BATCH = 40000  # 적절한 토큰 제한\n",
    "\n",
    "def batch_save(vectorstore, splits):\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for doc in splits: \n",
    "        tokens = len(encoding.encode(doc.page_content))\n",
    "        \n",
    "        if current_tokens + tokens > TOKEN_LIMIT_PER_BATCH:\n",
    "            try:\n",
    "                vectorstore.add_documents(current_batch)\n",
    "                print(f\"✅ {len(current_batch)}개 문서 배치 저장 완료\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 배치 저장 중 오류 발생: {e}\")\n",
    "            current_batch = [doc]\n",
    "            current_tokens = tokens\n",
    "        else:\n",
    "            current_batch.append(doc)\n",
    "            current_tokens += tokens\n",
    "\n",
    "    # 마지막 배치 처리\n",
    "    if current_batch:\n",
    "        vectorstore.add_documents(current_batch)\n",
    "        print(f\"✅ 마지막 배치 {len(current_batch)}개 문서 저장 완료\")\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "if not os.path.exists(persist_directory):\n",
    "    print(f\"Creating new Chroma store at {persist_directory}...\")    \n",
    "    vectorstore = Chroma(\n",
    "        embedding_function=embedding,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectorstore = batch_save(vectorstore, all_splits)\n",
    "else :\n",
    "    # 이미 존재하는 Chroma store를 불러오기\n",
    "    print(\"Loading existing Chroma store...\")\n",
    "    vectorstore = Chroma(\n",
    "        embedding_function=embedding,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    # 만약 새로 추가할 PDF 파일이 있다면 데이터 추출 및 저장후, extra_data 폴더에서 data 폴더로 이동\n",
    "    src_folder = 'extra_data'\n",
    "    dst_folder = 'data'\n",
    "\n",
    "    current_dir = os.getcwd() # 현재 폴더 경로\n",
    "    folder_path = os.path.join(current_dir, src_folder) # extra_data 폴더 경로 설정\n",
    "    pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\")) # PDF 파일 목록 가져오기\n",
    "\n",
    "    extra_splits = []\n",
    "    if not pdf_files:\n",
    "        print(\"추가할 PDF 파일이 없습니다.\")\n",
    "    else:\n",
    "        print(f\"추가할 PDF 파일 개수: {len(pdf_files)}\")\n",
    "        for pdf_file in pdf_files:\n",
    "            extra_splits.extend(extract_documents_from_pdf(pdf_file))\n",
    "\n",
    "        vectorstore = batch_save(vectorstore, extra_splits)\n",
    "\n",
    "        # extra_data 폴더의 모든 파일 중 .pdf만 이동\n",
    "        for filename in os.listdir(src_folder):\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                src_path = os.path.join(src_folder, filename)\n",
    "                dst_path = os.path.join(dst_folder, filename)\n",
    "                if os.path.isfile(src_path):\n",
    "                    shutil.move(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d5f822",
   "metadata": {},
   "source": [
    "## LLM 설정 후 질문 및 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ad7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "HF_API_KEY = os.getenv(\"HF_API_KEY\")\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_API_KEY\n",
    ")\n",
    "\n",
    "summary_completion = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-120b\"\n",
    ")\n",
    "print(summary_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571fe2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "if LLM_MODEL == \"gpt-4o-mini\":\n",
    "    chat = ChatOpenAI(model=LLM_MODEL)\n",
    "# elif LLM_MODEL == \"gpt-oss-120b\":\n",
    "#     HF_API_KEY = os.getenv(\"HF_API_KEY\")\n",
    "#     chat = ChatOpenAI(\n",
    "#         base_url=\"https://router.huggingface.co/v1\",\n",
    "#         api_key=HF_API_KEY,\n",
    "#         model = LLM_MODEL\n",
    "#     )\n",
    "\n",
    "question_answering_promt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Please write your answer in a markdown table format with the main points. Be sure to include all your source and page numbers like (3 ~ 10) in your answer. If you have over one source, you should include all of them. Answer in Korean. \\n#Example Format: \\n(brief summary of the answer) \\n (table) \\n  (detailed answer to the question) \\n**출처** \\n- (file source) (page source and page number) (Please write the quoted text within 20 characters and follow it with ... )\\n #Context: {context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(chat, question_answering_promt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef843e4",
   "metadata": {},
   "source": [
    "- meta data에 있는 정보는 llm이 알 수 없으므로 출처 filename을 포함시켜서 새로운 Document 타입을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f07455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Document type인 docs가 context로 넘겨졌을 때 llm은 content에 있는 정보 기반으로 답을 한다.\n",
    "# 따라서 meta data에 있는 정보는 llm이 알 수 없으므로 출처 filename을 포함시켜서 새로운 Document 타입을 생성한다.\n",
    "def format_docs_with_source_as_documents(docs):\n",
    "    new_docs = []\n",
    "    for d in docs:\n",
    "        filename = os.path.basename(d.metadata.get(\"source\", \"\"))\n",
    "        # 기존 page_content 뒤에 출처 붙이기\n",
    "        new_content = f\"{d.page_content}\\n출처: {filename}\"\n",
    "\n",
    "        # 새 리스트 생성 (metadata 유지)\n",
    "        new_docs.append(\n",
    "            Document(page_content=new_content, metadata=d.metadata)\n",
    "        )\n",
    "    return new_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69b8f1",
   "metadata": {},
   "source": [
    "### 질문 입력 및 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(k=3)\n",
    "question = \"우리 부부의 연소득은 총 합 6800만원이야. 받을 수 있는 대출은 뭐가 있을까?\"\n",
    "docs = retriever.invoke(question)\n",
    "# print(type(docs))\n",
    "# print(docs)\n",
    "formatted_context = format_docs_with_source_as_documents(docs)\n",
    "\n",
    "for d in formatted_context:\n",
    "    # print(d.metadata)\n",
    "    print(d)\n",
    "    print(\"-\" * 40)\n",
    "# for d in docs:\n",
    "#     print(d.metadata)\n",
    "#     print(d.page_content)\n",
    "#     print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "chat_history.add_user_message(question)\n",
    "\n",
    "answer = document_chain.invoke(\n",
    "    {\n",
    "        \"messages\": chat_history.messages,\n",
    "        \"context\": formatted_context,\n",
    "    }\n",
    ")\n",
    "\n",
    "chat_history.add_ai_message(answer)\n",
    "print(\"질문:\", question)\n",
    "print(\"답변:\")\n",
    "print(\"-\" * 40)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n청킹 방법\", splitter_name)\n",
    "print(\"임베딩 모델\", MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

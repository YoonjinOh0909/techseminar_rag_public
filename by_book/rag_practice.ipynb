{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5801b44f",
   "metadata": {},
   "source": [
    "# data에 있는 모든 문서 추출하기\n",
    "- chroma_store가 없을 경우 data 폴더에 있는 pdf 파일들을 읽어서 벡터 DB에 저장 하기 위한 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b46fcefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import utils\n",
    "utils.get_env_var.cache_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a10725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # 이건 그대로\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGSMITH_API_KEY # 이거 API키 받은거\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"lm-studio-rag-tracing\"  # 프로젝트 이름 설정(아무거나 해도됨)\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84d850",
   "metadata": {},
   "source": [
    "### 기본 방법 선택하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "33c88d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청킹 방법\n",
    "splitter_name = \"recursive\" # recursive, semantic\n",
    "\n",
    "# LLM 모델\n",
    "LLM_MODEL = \"gpt-4o-mini\" # gpt-4o-mini, openai/gpt-oss-120b\n",
    "\n",
    "# 벡터 DB\n",
    "VECTOR_DB = \"chroma_store\" # chroma_store, faiss_index\n",
    "\n",
    "# 임베딩 모델\n",
    "# EB_MODEL = \"text-embedding-3-large\" # text-embedding-3-large -> 아래 임베딩 모델 섹션에서 설정\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5b1e0",
   "metadata": {},
   "source": [
    "## 임베딩 모델 설정하기\n",
    "- 섹션이 바뀌기 전까지 셀 중 한가지 선택해서 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3fe06d",
   "metadata": {},
   "source": [
    "### OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cbee21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Embeddings : text-embedding-3-large\n",
    "# pip install langchain_openai\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 의미 기반으로 청킹을 하기 위해 OpenAI의 임베딩 모델을 사용\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "EB_MODEL = \"text-embedding-3-large\" # text-embedding-3-large\n",
    "embedding = OpenAIEmbeddings(model=EB_MODEL, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663e78c",
   "metadata": {},
   "source": [
    "### HuggingFaceEmbddings (local 저장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0c2af919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터 길이: 768\n",
      "임베딩 벡터 일부: [0.11608216911554337, -0.2838129997253418, 0.600308895111084, -0.2887226939201355, 0.6230913400650024]\n"
     ]
    }
   ],
   "source": [
    "# 로컬에 저장해서 사용하는거라 api key가 필요없음\n",
    "# HuggingFaceEmbeddings : sentence-transformers/stsb-xlm-r-multilingual\n",
    "\n",
    "# pip install langchain sentence-transformers langchain-community\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# HuggingFace 임베딩 클래스 이용\n",
    "EB_MODEL = \"stsb-xlm-r-multilingual\" # stsb-xlm-r-multilingual\n",
    "embedding = HuggingFaceEmbeddings(model_name=EB_MODEL)\n",
    "\n",
    "# 단일 텍스트 임베딩 생성\n",
    "sample_text = \"금융권 문서 임베딩 예제\"\n",
    "embedding_vector = embedding.embed_query(sample_text)\n",
    "\n",
    "print(\"임베딩 벡터 길이:\", len(embedding_vector))\n",
    "print(\"임베딩 벡터 일부:\", embedding_vector[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f0168",
   "metadata": {},
   "source": [
    "### Hugginface embeddings (multilinual-e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a6257139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터 길이: 1024\n",
      "임베딩 벡터 일부: [0.02028062380850315, 0.00727831618860364, -0.001797395758330822, -0.037471067160367966, 0.013377872295677662]\n"
     ]
    }
   ],
   "source": [
    "# pip install langchain_huggingface\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "EB_MODEL = \"intfloat/multilingual-e5-large-instruct\"  # multilingual-e5-large, multilingual-e5-large-instruct\n",
    "model_name = EB_MODEL\n",
    "\n",
    "HF_API_KEY = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "embedding = HuggingFaceEndpointEmbeddings(\n",
    "    model=model_name,\n",
    "    task=\"feature-extraction\",\n",
    "    huggingfacehub_api_token=HF_API_KEY,\n",
    ")\n",
    "\n",
    "# 단일 텍스트 임베딩 생성\n",
    "sample_text = \"금융권 문서 임베딩 예제\"\n",
    "embedding_vector = embedding.embed_query(sample_text)\n",
    "\n",
    "print(\"임베딩 벡터 길이:\", len(embedding_vector))\n",
    "print(\"임베딩 벡터 일부:\", embedding_vector[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b91604",
   "metadata": {},
   "source": [
    "### Google Gemini Embeddings\n",
    "- *호환성 문제로 genai 는 사용하지 않도록 한다.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471043c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터 길이: 768\n",
      "임베딩 벡터 일부: [-0.017458646, 0.023586035, 0.0074004014, -0.09534135, 0.011647189]\n"
     ]
    }
   ],
   "source": [
    "# # pip install genai google google.genai\n",
    "# from langchain.embeddings.base import Embeddings\n",
    "# from google import genai\n",
    "# from google.genai.types import EmbedContentConfig\n",
    "\n",
    "# api_key = os.getenv(\"GC_API_KEY\")\n",
    "\n",
    "# client = genai.Client(api_key=api_key)\n",
    "# EB_MODEL = \"gemini-embedding-001\"  # Google Cloud 임베딩 모델\n",
    "# def call_google_cloud_embedding_api(text):\n",
    "#     response = client.models.embed_content(\n",
    "#         model=EB_MODEL,\n",
    "#         contents=text,\n",
    "#         config=EmbedContentConfig(\n",
    "#             task_type=\"RETRIEVAL_DOCUMENT\",\n",
    "#             output_dimensionality=768\n",
    "#         )\n",
    "#     )\n",
    "#     return response.embeddings[0].values\n",
    "\n",
    "# class GoogleCloudEmbeddings(Embeddings):\n",
    "#     def embed_documents(self, texts):\n",
    "#         # Google Cloud 임베딩 API 호출 코드\n",
    "#         # texts 리스트를 임베딩 벡터 리스트로 반환\n",
    "#         embeddings = []\n",
    "#         for text in texts:\n",
    "#             embedding_vector = call_google_cloud_embedding_api(text)\n",
    "#             embeddings.append(embedding_vector)\n",
    "#         return embeddings\n",
    "\n",
    "#     def embed_query(self, text):\n",
    "#         return call_google_cloud_embedding_api(text)\n",
    "\n",
    "# # 생성\n",
    "# embedding = GoogleCloudEmbeddings()\n",
    "\n",
    "# sample_text = \"금융권 문서 임베딩 예제\"\n",
    "# embedding_vector = embedding.embed_query(sample_text)\n",
    "\n",
    "# print(\"임베딩 벡터 길이:\", len(embedding_vector))\n",
    "# print(\"임베딩 벡터 일부:\", embedding_vector[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a09bcc",
   "metadata": {},
   "source": [
    "### KF-DeBERTa embedding 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ef909454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터 길이: 768\n",
      "임베딩 벡터 일부: [ 0.84986967 -0.74095684  0.1062606  -0.3817094   0.35903543]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "EB_MODEL = \"kakaobank/kf-deberta-base\"  # KF-DeBERTa 모델 이름\n",
    "# KF-DeBERTa 임베딩 래퍼 정의\n",
    "class KFDeBERTaEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=EB_MODEL):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            embedding = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return embedding[0].cpu().numpy()\n",
    "\n",
    "# 생성\n",
    "embedding = KFDeBERTaEmbeddings()\n",
    "\n",
    "sample_text = \"금융권 문서 임베딩 예제\"\n",
    "embedding_vector = embedding.embed_query(sample_text)\n",
    "\n",
    "print(\"임베딩 벡터 길이:\", len(embedding_vector))\n",
    "print(\"임베딩 벡터 일부:\", embedding_vector[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97b62b",
   "metadata": {},
   "source": [
    "### bge-m3 Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f065864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "EB_MODEL = \"BAAI/bge-m3\"\n",
    "# 임베딩 모델 생성\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=EB_MODEL,\n",
    "    model_kwargs={\"device\": \"cpu\"},  # GPU 사용 시\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# sample_text = \"금융권 문서 임베딩 예제\"\n",
    "# embedding_vector = embedding.embed_query(sample_text)\n",
    "\n",
    "# print(\"임베딩 벡터 길이:\", len(embedding_vector))\n",
    "# print(\"임베딩 벡터 일부:\", embedding_vector[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b1441",
   "metadata": {},
   "source": [
    "## 청킹 방법 설정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93116c7",
   "metadata": {},
   "source": [
    "### splitter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5e0503fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x000001F3304DF640> \n",
      "embedding:  BAAI/bge-m3\n"
     ]
    }
   ],
   "source": [
    "# pip install langchain_experimental\n",
    "if splitter_name == \"recursive\" :\n",
    "    # recursive_character_text_splitter로 구분자 재귀 청킹 기법\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "elif splitter_name == \"semantic\":\n",
    "    # pip install langchain_experimental\n",
    "    from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "    text_splitter = SemanticChunker(embedding)\n",
    "\n",
    "print(text_splitter, \"\\nembedding: \", EB_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c63422",
   "metadata": {},
   "source": [
    "### Debug 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "78f42103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_chunkinfo_aftersplit(all_splits):\n",
    "    for i, split in enumerate(all_splits):\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        print(split.page_content)\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17918d6",
   "metadata": {},
   "source": [
    "### 추출 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b594f8",
   "metadata": {},
   "source": [
    "- pdf에서 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1cf92787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일을 읽어서 텍스트 데이터 추출 및 청킹\n",
    "def extract_documents_from_pdf(pdf_path):\n",
    "    # PDF 파일을 읽어서 텍스트 데이터 추출\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    data_nyc = loader.load()\n",
    "\n",
    "    # 추출된 텍스트 데이터를 청킹\n",
    "    splits = text_splitter.split_documents(data_nyc)\n",
    "    # debug_chunkinfo_aftersplit(splits) \n",
    "\n",
    "    # recursive_character_text_splitter의 경우, 청크가 겹치는 부분이 없으면 연결하지 않음\n",
    "    # 따라서, 청크가 겹치는 부분이 없을 때는 직접 연결하여 오버랩을 만듦\n",
    "    # 만약 청크가 겹치는 부분이 있다면, 그 부분은 자동으로 연결됨\n",
    "    # 여기서는 청크가 겹치지 않는 경우에만 오버랩을 추가함\n",
    "\n",
    "    # 만약 첫 번째 청크의 끝과 두 번째 청크의 시작이 겹치지 않는다면,\n",
    "    if splitter_name == \"recursive\" and (splits[0].page_content[-100:] == splits[1].page_content[:100]):\n",
    "        print(splits[0].page_content)\n",
    "        print(\"----\")\n",
    "        print(splits[1].page_content)\n",
    "        for i in range(len(splits) - 1):\n",
    "            splits[i].page_content += \"\\n\" + splits[i + 1].page_content[:50]\n",
    "\n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87940ce",
   "metadata": {},
   "source": [
    "## 텍스트를 벡터로 변환하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca0cc98",
   "metadata": {},
   "source": [
    "- 토큰 제한하며 실행 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "26ea0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pypdf\n",
    "import tiktoken\n",
    "import pypdf\n",
    "\n",
    "try:\n",
    "    encoding = tiktoken.encoding_for_model(EB_MODEL)\n",
    "except KeyError:\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\") # 토크나이저를 직접 지정\n",
    "\n",
    "\n",
    "TOKEN_LIMIT_PER_BATCH = 39000  # 적절한 토큰 제한\n",
    "\n",
    "def batch_save(vectorstore, splits):\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for doc in splits: \n",
    "        tokens = len(encoding.encode(doc.page_content))\n",
    "        \n",
    "        if current_tokens + tokens > TOKEN_LIMIT_PER_BATCH:\n",
    "            try:\n",
    "                vectorstore.add_documents(current_batch)\n",
    "                print(f\"✅ {len(current_batch)}개 문서 배치 저장 완료\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 배치 저장 중 오류 발생: {e}\")\n",
    "            current_batch = [doc]\n",
    "            current_tokens = tokens\n",
    "        else:\n",
    "            current_batch.append(doc)\n",
    "            current_tokens += tokens\n",
    "\n",
    "    # 마지막 배치 처리\n",
    "    if current_batch:\n",
    "        vectorstore.add_documents(current_batch)\n",
    "        print(f\"✅ 마지막 배치 {len(current_batch)}개 문서 저장 완료\")\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049696b",
   "metadata": {},
   "source": [
    "### Vector DB 생성 혹은 데이터 불러오기 (Chroma, FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1570fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing chroma_store...\n",
      "추가할 PDF 파일이 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# pip install faiss-cpu\n",
    "import faiss\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_chroma import Chroma\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "persist_directory = './' + VECTOR_DB + '_' + splitter_name + '_' + EB_MODEL.replace('/', '_')\n",
    "\n",
    "if not os.path.exists(persist_directory):\n",
    "    print(f\"Creating new {VECTOR_DB} at {persist_directory}...\")    \n",
    "\n",
    "    # 만약 persist_directory가 존재하지 않는다면, 새로 생성. data에 있는 PDF 파일을 읽어서 청킹 후 저장\n",
    "    current_dir = os.getcwd() # 현재 폴더 경로\n",
    "    folder_path = os.path.join(current_dir, \"data\") # data 폴더 경로 설정\n",
    "    pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\")) # PDF 파일 목록 가져오기\n",
    "\n",
    "    all_splits = []\n",
    "    print(f\"현재 청킹 방법 : {splitter_name}\")\n",
    "    print(f\"현재 임베딩 모델 : {EB_MODEL}\")\n",
    "    print(f\"PDF 파일 개수: {len(pdf_files)}\")\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = pdf_file\n",
    "        # temp_docs, merged_path = extract_documents_from_pdf(pdf_file)\n",
    "        print(f\"Processing {pdf_path}...\")\n",
    "        all_splits.extend(extract_documents_from_pdf(pdf_file))\n",
    "        time.sleep(2)  # PDF 파일 처리 간에 잠시 대기\n",
    "\n",
    "    print(f\"전체 청크 개수: {len(all_splits)}\")\n",
    "\n",
    "    if VECTOR_DB == \"chroma_store\":\n",
    "        vectorstore = Chroma(\n",
    "            embedding_function=embedding,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        vectorstore = batch_save(vectorstore, all_splits)\n",
    "    elif VECTOR_DB == \"faiss_index\":\n",
    "    \n",
    "        embedding_dim = len(embedding.embed_query(\"test\"))\n",
    "        index = faiss.IndexFlatIP(embedding_dim)\n",
    "\n",
    "        vectorstore = FAISS(\n",
    "            embedding_function=embedding.embed_query,\n",
    "            index=index,\n",
    "            docstore=InMemoryDocstore(),\n",
    "            index_to_docstore_id={}\n",
    "        )\n",
    "        vectorstore = batch_save(vectorstore, all_splits)\n",
    "        vectorstore.save_local(persist_directory)\n",
    "else :\n",
    "    # 이미 존재하는 Chroma store를 불러오기\n",
    "    print(f\"Loading existing {VECTOR_DB}...\")\n",
    "    if VECTOR_DB == \"faiss_index\":\n",
    "        vectorstore = FAISS.load_local(persist_directory, embedding, allow_dangerous_deserialization=True)\n",
    "    elif VECTOR_DB == \"chroma_store\":\n",
    "        vectorstore = Chroma(\n",
    "            embedding_function=embedding,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "    # 만약 새로 추가할 PDF 파일이 있다면 데이터 추출 및 저장후, extra_data 폴더에서 data 폴더로 이동\n",
    "    src_folder = 'extra_data'\n",
    "    dst_folder = 'data'\n",
    "\n",
    "    current_dir = os.getcwd() # 현재 폴더 경로\n",
    "    folder_path = os.path.join(current_dir, src_folder) # extra_data 폴더 경로 설정\n",
    "    pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\")) # PDF 파일 목록 가져오기\n",
    "\n",
    "    extra_splits = []\n",
    "    if not pdf_files:\n",
    "        print(\"추가할 PDF 파일이 없습니다.\")\n",
    "    else:\n",
    "        print(f\"추가할 PDF 파일 개수: {len(pdf_files)}\")\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            extra_splits.extend(extract_documents_from_pdf(pdf_file))\n",
    "            time.sleep(1)\n",
    "\n",
    "        vectorstore = batch_save(vectorstore, extra_splits)\n",
    "        if VECTOR_DB == \"faiss_index\":\n",
    "            vectorstore.save_local(persist_directory)\n",
    "        # extra_data 폴더의 모든 파일 중 .pdf만 이동\n",
    "        for filename in os.listdir(src_folder):\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                src_path = os.path.join(src_folder, filename)\n",
    "                dst_path = os.path.join(dst_folder, filename)\n",
    "                if os.path.isfile(src_path):\n",
    "                    shutil.move(src_path, dst_path)\n",
    "\n",
    "# semantic text-embedding-3-large chroma 12m 23.7s 청크 개수 342\n",
    "# semantic stsb-xlm-r-multilingual chroma 2m 38.5s 청크 개수 342\n",
    "# semantic intfloat/multilingual-e5-large-instruct chroma 6m 29.6s 청크 개수 342\n",
    "# semantic kakaobank/kf-deberta-base chroma 9m 53.6s 청크 개수 342\n",
    "# semantic BAAI/bge-m3 chroma 31m 48.9s 청크 개수 342\n",
    "\n",
    "\n",
    "\n",
    "# semantic text-embedding-3-large faiss_index 12m 21.9s 청크 개수 342\n",
    "# semantic stsb-xlm-r-multilingual faiss_index 3m 35.2s 청크 개수 342\n",
    "# semantic intfloat/multilingual-e5-large-instruct faiss_index 7m 54.1s 청크 개수 342\n",
    "# semantic kakaobank/kf-deberta-base faiss_index 9m 53.4s 청크 개수 342\n",
    "# semantic BAAI/bge-m3 faiss_index 25m 3.8s 청크 개수 342"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d5f822",
   "metadata": {},
   "source": [
    "## LLM 설정 후 질문 및 답변"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d5f87",
   "metadata": {},
   "source": [
    "### gpt-4o-mini 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "571fe2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_openai import ChatOpenAI # 라고 했을 때는 module 'openai' has no attribute 'DefaultHttpxClient' 오류가\n",
    "from openai import OpenAI\n",
    "\n",
    "if LLM_MODEL == \"gpt-4o-mini\":\n",
    "    chat = ChatOpenAI(model=LLM_MODEL)\n",
    "# elif LLM_MODEL == \"gpt-oss-120b\":\n",
    "#     HF_API_KEY = os.getenv(\"HF_API_KEY\")\n",
    "#     chat = ChatOpenAI(\n",
    "#         base_url=\"https://router.huggingface.co/v1\",\n",
    "#         api_key=HF_API_KEY,\n",
    "#         model = LLM_MODEL\n",
    "#     )\n",
    "\n",
    "question_answering_promt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            # \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Please write your answer in a markdown table format with the main points. Be sure to include all your source and page numbers like (3 ~ 10) in your answer. If you have over one source, you should include all of them. Answer in Korean. \\n#Example Format: \\n(brief summary of the answer) \\n (table) \\n  (detailed answer to the question) \\n**출처** \\n- (file source) (page source and page number) (Please write the quoted text within 20 characters and follow it with ... )\\n #Context: {context}\",\n",
    "            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. you should user performed_context. You should consider all of special situation and general situation. If the 'perforemd_context' is null, you just say 'it is empyt'. Please write your answer in a markdown table format with the main points. Be sure to include all your source and page numbers like (3 ~ 10) in your answer. If you have over one source, you should include all of them. Answer in Korean. Also please write the keywords on user question that you think. \\n#Example Format: \\n(brief summary of the answer) \\n (table) \\n  (detailed answer to the question) \\n**출처** \\n- (file source) (page source and page number) (Please write the quoted text within 20 characters and follow it with ... )\\n\\n 키워드 : (keywords)\\n #Context: {context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(chat, question_answering_promt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef843e4",
   "metadata": {},
   "source": [
    "- meta data에 있는 정보는 llm이 알 수 없으므로 출처 filename을 포함시켜서 새로운 Document 타입을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "43f07455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Document type인 docs가 context로 넘겨졌을 때 llm은 content에 있는 정보 기반으로 답을 한다.\n",
    "# 따라서 meta data에 있는 정보는 llm이 알 수 없으므로 출처 filename을 포함시켜서 새로운 Document 타입을 생성한다.\n",
    "def format_docs_with_source_as_documents(docs):\n",
    "    new_docs = []\n",
    "    for d in docs:\n",
    "        filename = os.path.basename(d.metadata.get(\"source\", \"\"))\n",
    "        # 기존 page_content 뒤에 출처 붙이기\n",
    "        new_content = f\"{d.page_content}\\n출처: {filename}\"\n",
    "\n",
    "        # 새 리스트 생성 (metadata 유지)\n",
    "        new_docs.append(\n",
    "            Document(page_content=new_content, metadata=d.metadata)\n",
    "        )\n",
    "    return new_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69b8f1",
   "metadata": {},
   "source": [
    "### 질문 입력 및 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "55a65edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['연소득', '6800만원', '군인', '직장인', '대출', '부부']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "HF_API_KEY = os.getenv(\"HF_API_KEY\")\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_API_KEY\n",
    ")\n",
    "question = \"우리 부부의 연소득은 총 합 6800만원이야. 한 명은 군에 종사하고 하나는 직장인이라는 것을 참고해줘. 받을 수 있는 대출은 뭐가 있을까?\"\n",
    "\n",
    "# \"우리 부부의 연소득은 총 합 6800만원이야. 한 명은 군에 종사하고 하나는 직장인이라는 것을 참고해줘. 받을 수 있는 대출은 뭐가 있을까?\"\n",
    "# '\"연소득\", \"6800만원\", \"군인\", \"직장인\", \"대출\", \"부부\"'\n",
    "# \"우리 부부의 연소득은 총 합 6800만원이야. 한 명은 군에 종사하고 하나는 직장인이라는 것을 참고해줘. 받을 수 있는 대출은 뭐가 있을까?\"\n",
    "# '\"연소득\", \"6800만원\", \"군인\", \"직장인\", \"대출\", \"부부\"'\n",
    "# \"우리 부부의 연소득은 총 합 6800만원이야. 한 명은 군에 종사하고 하나는 직장인이라는 것을 참고해줘. 받을 수 있는 대출은 뭐가 있을까?\"\n",
    "# \"우리 부부는 한 명은 군인이고 한 명은 직장인이야. 전세 때문에 대출을 받으려고해. 둘 합산 연소득은 6000만원이야 받을 수 있는 대출은 뭐가 있지? 모두 알려줘\"\n",
    "# \"우리 부부의 연소득은 총 합 6800만원이야. 받을 수 있는 대출은 뭐가 있을까?\"\n",
    "\n",
    "# print(formatted_context)\n",
    "summary_completion = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": f\"You are an assistant for question-answering tasks. 벡터 db에 넣기 위해 주어진 질문의 키워드를 파이썬 리스트 형태로 답변해줘. 백틱도 안 넣어도돼. The question is : {question}\"\n",
    "        }],\n",
    ")\n",
    "# print(\"질문:\", question)\n",
    "# print(\"답변:\")\n",
    "# print(\"-\" * 40)\n",
    "\n",
    "question_for_vectordb = summary_completion.choices[0].message.content\n",
    "print(question_for_vectordb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "59c6572d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ITStudy\\Project\\TechSeminar_Public\\by_book\\data\\우리 군인우대 대출.pdf\n",
      "----------------------------------------\n",
      "c:\\ITStudy\\Project\\TechSeminar_Public\\by_book\\data\\우리 군인우대 대출.pdf\n",
      "----------------------------------------\n",
      "c:\\ITStudy\\Project\\TechSeminar_Public\\by_book\\data\\boguem_howto.pdf\n",
      "----------------------------------------\n",
      "c:\\ITStudy\\Project\\TechSeminar_Public\\by_book\\data\\bogeum_guidline.pdf\n",
      "----------------------------------------\n",
      "c:\\ITStudy\\Project\\TechSeminar_Public\\by_book\\data\\우리 사잇돌 중금리대출.pdf\n",
      "----------------------------------------\n",
      "c:\\ITStudy\\Project\\TechSeminar_Public\\by_book\\data\\bogeum_guidline.pdf\n",
      "----------------------------------------\n",
      "c:\\ITStudy\\Project\\TechSeminar_Public\\by_book\\data\\디딤돌대출_업무처리기준.pdf\n",
      "----------------------------------------\n",
      "c:\\ITStudy\\Project\\TechSeminar_Public\\by_book\\data\\우리 군인우대 대출.pdf\n",
      "----------------------------------------\n",
      "c:\\ITStudy\\Project\\TechSeminar_Public\\by_book\\data\\boguem_howto.pdf\n",
      "----------------------------------------\n",
      "c:\\ITStudy\\Project\\TechSeminar_Public\\by_book\\data\\디딤돌대출_업무처리기준.pdf\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 10})\n",
    "# k를 3개에서 10개로 변경\n",
    "\n",
    "docs = retriever.invoke(question_for_vectordb)\n",
    "# print(type(docs))\n",
    "# print(docs)\n",
    "formatted_context = format_docs_with_source_as_documents(docs)\n",
    "\n",
    "for d in formatted_context:\n",
    "    print(d.metadata[\"source\"])\n",
    "    # print(d)\n",
    "    print(\"-\" * 40)\n",
    "# for d in docs:\n",
    "#     print(d.metadata)\n",
    "#     print(d.page_content)\n",
    "#     print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48fe9b",
   "metadata": {},
   "source": [
    "### openai/gpt-oss-120b 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "900689de",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIStatusError",
     "evalue": "Error code: 402 - {'error': 'You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[194], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[0;32m      5\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://router.huggingface.co/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mHF_API_KEY\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# print(formatted_context)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m summary_completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai/gpt-oss-120b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt know the answer, just say that you don\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt know. you should user performed_context. You should consider all of special situation and general situation. If the \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mperforemd_context\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m is null, you just say \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mit is empyt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m. Please write your answer in a markdown table format with the main points. Be sure to include all your source and page numbers like (3 ~ 10) in your answer. If you have over one source, you should include all of them. Answer in Korean. Also please write the keywords on user question that you think. \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m#Example Format: \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m(brief summary of the answer) \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m (table) \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  (detailed answer to the question) \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m**출처** \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m- (file source) (page source and page number) (Please write the quoted text within 20 characters and follow it with ... )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m 키워드 : (keywords)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m #Context: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mformatted_context\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m. The question is : \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mquestion\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# print(\"답변:\")\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# print(\"-\" * 40)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# print(\"질문:\", question)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|분류|종류|\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ITStudy\\Project\\TechSeminar_Public\\.venv\\lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ITStudy\\Project\\TechSeminar_Public\\.venv\\lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1153\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m   1151\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m   1152\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m-> 1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ITStudy\\Project\\TechSeminar_Public\\.venv\\lib\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\ITStudy\\Project\\TechSeminar_Public\\.venv\\lib\\site-packages\\openai\\_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAPIStatusError\u001b[0m: Error code: 402 - {'error': 'You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.'}"
     ]
    }
   ],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "HF_API_KEY = os.getenv(\"HF_API_KEY\")\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_API_KEY\n",
    ")\n",
    "# print(formatted_context)\n",
    "summary_completion = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": f\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. you should user performed_context. You should consider all of special situation and general situation. If the 'perforemd_context' is null, you just say 'it is empyt'. Please write your answer in a markdown table format with the main points. Be sure to include all your source and page numbers like (3 ~ 10) in your answer. If you have over one source, you should include all of them. Answer in Korean. Also please write the keywords on user question that you think. \\n#Example Format: \\n(brief summary of the answer) \\n (table) \\n  (detailed answer to the question) \\n**출처** \\n- (file source) (page source and page number) (Please write the quoted text within 20 characters and follow it with ... )\\n\\n 키워드 : (keywords)\\n #Context: {formatted_context}. The question is : {question}\"\n",
    "        }],\n",
    ")\n",
    "\n",
    "# print(\"답변:\")\n",
    "# print(\"-\" * 40)\n",
    "\n",
    "# print(\"질문:\", question)\n",
    "print(\"|분류|종류|\")\n",
    "print(\"|---|---|\")\n",
    "print(\"|청킹 방법|\", splitter_name, \"|\")\n",
    "print(\"|임베딩 모델|\", EB_MODEL, \"|\")\n",
    "print(\"|벡터 DB|\", VECTOR_DB, \"|\")\n",
    "print(\"|LLM 모델|\", \"openai/gpt-oss-120b\", \"|\\n\")\n",
    "\n",
    "print(\"답변 : \")\n",
    "\n",
    "print(summary_completion.choices[0].message.content)\n",
    "print(\"\\n\\n\")\n",
    "print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e83fd",
   "metadata": {},
   "source": [
    "### lanchain 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "69df40c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|분류|종류|\n",
      "|---|---|\n",
      "|청킹 방법| recursive |\n",
      "|임베딩 모델| kakaobank/kf-deberta-base |\n",
      "|벡터 DB| chroma_store |\n",
      "|LLM 모델| gpt-4o-mini |\n",
      "\n",
      "답변 : \n",
      "| 대출 종류            | 대출 대상                          | 대출 한도                  | 대출 조건                                    |\n",
      "|----------------------|-----------------------------------|---------------------------|----------------------------------------------|\n",
      "| 디딤돌 대출         | 연소득 6800만원 이하 가구         | 최대 85백만원               | 신혼가구, 자녀가구, 다자녀가구 등 조건 있음  |\n",
      "| 청년도약대출       | 만 34세 이하 연소득 4000만원 이하 | 최대 5백만원               | 해당 사항 없음 (재직기간 3개월 이상 필요)    |\n",
      "| 참사랑 대출         | 근로복지공단 승인 고객            | 융자결정기관에 의해 확정됨| 산재근로자 및 자녀 학자금 등                  |\n",
      "\n",
      "대출의 종류에 따라 다르지만, 부부의 연소득이 6800만원이라면 디딤돌 대출과 같은 상품을 고려할 수 있습니다. \n",
      "\n",
      "**출처** \n",
      "- 우리은행_주택담보대출 상품설명서(변경 후) .pdf (3~10) \"연소득 6800만원 이하...\"  \n",
      "- 디딤돌대출_업무처리기준.pdf (3~10) \"최대 85백만원...\"  \n",
      "- 산재근로자 참사랑대출.pdf (3~10) \"융자결정기관에 의해 확정됨...\"  \n",
      "\n",
      "키워드: 연소득, 대출 종류, 군인, 직장인, 조건, 디딤돌 대출, 청년도약대출\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "chat_history.add_user_message(question)\n",
    "\n",
    "answer = document_chain.invoke(\n",
    "    {\n",
    "        \"messages\": chat_history.messages,\n",
    "        \"context\": formatted_context,\n",
    "    }\n",
    ")\n",
    "\n",
    "chat_history.add_ai_message(answer)\n",
    "\n",
    "print(\"|분류|종류|\")\n",
    "print(\"|---|---|\")\n",
    "print(\"|청킹 방법|\", splitter_name, \"|\")\n",
    "print(\"|임베딩 모델|\", EB_MODEL, \"|\")\n",
    "print(\"|벡터 DB|\", VECTOR_DB, \"|\")\n",
    "print(\"|LLM 모델|\", LLM_MODEL, \"|\\n\")\n",
    "\n",
    "print(\"답변 : \")\n",
    "\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad6218",
   "metadata": {},
   "source": [
    "### context 사용 없이 질문을 받았을 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9039a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 우리 부부는 한 명은 군에 종사하고 있고, 한 명은 직장인이야. 전세 때문에 대출을 받으려고해. 둘 합산 연소득은 6800만원이야 받을 수 있는 대출은 뭐가 있지? 모두 알려줘\n",
      "답변:\n",
      "----------------------------------------\n",
      "**요약**: 연소득 6,800만원이면 전세자금대출·주택담보대출·신용대출 등을 활용해 2 ~ 3억 원까지 대출이 가능해요.  \n",
      "\n",
      "|대출 종류|주요 조건|예상 한도(최대)|\n",
      "|---|---|---|\n",
      "|전세자금 대출|소득·신용·주택가격·전세보증금 기준, LTV 80%·DTI 45% 이하|전세보증금 80%·≈2.5억|\n",
      "|주택담보대출|주택소유·LTV 70%·DTI 50% 이하|주택가치 70%·≈3억|\n",
      "|신용(일반)대출|소득·신용점수·부채비율 ≤ 60%|연소득 30%·≈2억|\n",
      "\n",
      "전세자금 대출은 은행·주택금융공사가 공동으로 제공하고, 주택담보대출은 보유 주택이 있을 경우 LTV·DTI에 따라 한도가 결정됩니다. 신용대출은 부채·신용점수에 따라 연소득 30% 정도까지 제한됩니다.\n",
      "\n",
      "**출처**  \n",
      "- 금융감독원, “주택대출·DTI·LTV 기준” (2024) https://fss.or.kr/... (“LTV 80%·DTI 45%...” )  \n",
      "- KB국민은행, “전세자금 대출상품 안내” https://obank.kbstar.com/... (“전세보증금 80%…” )\n",
      "\n",
      "청킹 방법 context 사용 없음\n",
      "임베딩 모델 context 사용 없음\n",
      "LLM 모델 openai/gpt-oss-120b\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "HF_API_KEY = os.getenv(\"HF_API_KEY\")\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_API_KEY\n",
    ")\n",
    "# print(formatted_context)\n",
    "summary_completion = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": f\"You are an assistant for question-answering tasks. Please keep the answer under 500 characters. Please write your answer in a markdown table format with the main points.  Answer in Korean. \\n#Example Format: \\n(brief summary of the answer) \\n (table) \\n  (detailed answer to the question) \\n**출처** \\n- (file source) (page source and page number and correct full url) (Please write the quoted text within 20 characters and follow it with ... )\\n The question is : {question}\"\n",
    "        }],\n",
    ")\n",
    "print(\"\\n청킹 방법\", \"context 사용 없음\")\n",
    "print(\"임베딩 모델\", \"context 사용 없음\")\n",
    "print(\"LLM 모델\", \"openai/gpt-oss-120b\")\n",
    "print(\"벡터 DB\", \"사용 없음\")\n",
    "\n",
    "print(\"답변 : \")\n",
    "print(\"-\" * 40)\n",
    "print(summary_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b1b86",
   "metadata": {},
   "source": [
    "## 질의 확장 구현\n",
    "- 챗본으로 만들 경우 첫번째 질문을 이어 받아서 두번째 질문도 답변을 해야하니 질의 확장에 대한 기능이다. (필수 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "565825aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부부의 연소득이 3000만원일 때 받을 수 있는 대출은 무엇인가요?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "query_for_other = \"3천만원 일때는?\"\n",
    "\n",
    "query_augmentation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"기존의 대화 내용을 활용하여 사용자가 질문한 의도를 파악해서 한 문장의 명료한 질문으로 변환하라. 대명사나 이, 저, 그와 같은 표현을 명확한 명사로 표현하라. : \\n\\n{query}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "query_augmentation_chain = query_augmentation_prompt | chat | StrOutputParser()\n",
    "\n",
    "augmented_query = query_augmentation_chain.invoke(\n",
    "    {\n",
    "        \"messages\": chat_history.messages,\n",
    "        \"query\": query_for_other,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(augmented_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076f592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 3천만원 일때는?\n",
      "답변:\n",
      "----------------------------------------\n",
      "연소득이 3000만원일 때의 대출 가능성은 다음과 같이 요약할 수 있습니다.\n",
      "\n",
      "| 주요 내용               | 세부 사항                                     |\n",
      "|----------------------|--------------------------------------------|\n",
      "| 연소득               | 3000만원                                    |\n",
      "| 대출한도 — 일반      | 2억원 이내                                   |\n",
      "| 대출한도 — 생애최초  | 2.4억원                                     |\n",
      "| 대출한도 — 신혼·다자녀 | 3.2억원                                     |\n",
      "| 소득 추정 기준       | 5천만원 이하 대출 취급 가능                 |\n",
      "\n",
      "부부의 연소득이 3000만원인 경우, 일반적인 대출한도는 최대 2억원입니다. 또한 생애 최초 주택 구입자나 신혼 및 다자녀 가구의 경우 각각 2.4억원과 3.2억원까지 대출 가능성이 있습니다. 소득이 5천만원 이하이므로 대출 신청이 가능하다는 점 또한 주요합니다.\n",
      "\n",
      "**출처**\n",
      "- 디딤돌대출_업무처리기준.pdf (22페이지) \"소득추정 금액이 60백만원 이하 ...\" \n",
      "\n",
      "청킹 방법 recursive\n",
      "임베딩 모델 text-embedding-3-large\n",
      "LLM 모델 gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "docs_other = retriever.invoke(augmented_query)\n",
    "# print(type(docs))\n",
    "# print(docs)\n",
    "formatted_context_other = format_docs_with_source_as_documents(docs_other)\n",
    "\n",
    "chat_history.add_user_message(query_for_other)\n",
    "\n",
    "answer = document_chain.invoke(\n",
    "    {\n",
    "        \"messages\": chat_history.messages,\n",
    "        \"context\": formatted_context_other,\n",
    "    }\n",
    ")\n",
    "\n",
    "chat_history.add_ai_message(answer)\n",
    "\n",
    "print(\"질문:\", query_for_other)\n",
    "print(\"답변:\")\n",
    "print(\"-\" * 40)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n청킹 방법\", splitter_name)\n",
    "print(\"임베딩 모델\", EB_MODEL)\n",
    "print(\"LLM 모델\", LLM_MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
